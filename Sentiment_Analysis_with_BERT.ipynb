{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OT2nCA-M9X6"
      },
      "source": [
        "# Sentiment Analysis: Sentiment classification using a pretrained fine tuned BERT model\n",
        "- Refer to [BERT_Training](https://colab.research.google.com/drive/1FicAdDkFhe0rRobwbBSonmuNaSVFB3iZ?usp=sharing) notebook for trainig the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd0xr4WHepDH"
      },
      "source": [
        "# NOTES and REFRENCES:\n",
        "- This notebook can only be run on the [Colab](https://colab.research.google.com/drive/1HABn0JGshF2TJGZmL061ABJgy8BvY1GE?usp=sharing). Running this notebook require usage of GPU runtime. Set runtime to GPU by going to menue bar \"Runtime/change runtime\" and select GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifAPOAO5kuCL"
      },
      "source": [
        "Starting GPU, verify Runtime/runtime type==GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3gggEWQ1M1V",
        "outputId": "a1ebdb38-d3ad-4683-8c5f-3a5a9df1ca48",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Nov 18 00:09:26 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAKSb2WVvAAV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ph-Z46ny89I",
        "outputId": "cb7dfc93-fbf1-4b8c-cac2-9041adc0a370",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.3MB 12.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 56.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 51.6MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exRXb3i4zQqR",
        "outputId": "3e2f45e9-875b-4e5e-e319-e055899fa160",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPython 3.6.9\n",
            "IPython 5.5.0\n",
            "\n",
            "numpy 1.18.5\n",
            "pandas 1.1.4\n",
            "torch 1.7.0+cu101\n",
            "transformers 3.5.1\n"
          ]
        }
      ],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y004qNp-zsfK",
        "outputId": "978f5aaf-fdf3-49bc-b145-02211f25ac56",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 8,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "import re\n",
        "import io\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "random_seed = 62\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJBu7QdfI6lH"
      },
      "source": [
        "## Import Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE3p-2M41H4j",
        "outputId": "1bc0c918-a439-42bd-8d9b-9caed986e17e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Un-commenet to Mount your google drive that contains the raw tweet data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Or alternatively uncomment this line to upload the raw tweet dataset (156 files)\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgoYK02h4Qzg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "T89gGjODMFW1",
        "outputId": "af66193d-cb78-4186-a2b9-5ad9debd8312",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>2020-08-04.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>2020-08-05.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>2020-08-06.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>2020-08-07.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>dirlist.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0\n",
              "152  2020-08-04.csv\n",
              "153  2020-08-05.csv\n",
              "154  2020-08-06.csv\n",
              "155  2020-08-07.csv\n",
              "156     dirlist.txt"
            ]
          },
          "execution_count": 32,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Use this cell if tweet dataset is being read from drive ( Note:path variable should point to the location of dataset in drive)\n",
        "path = '/content/gdrive/My Drive/DVA/output/dirlist.csv'\n",
        "days = pd.read_csv(path,header=None)\n",
        "days.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKgwB8iOsfUR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## Use this cell if tweet dataset is uploaded\n",
        "# days = pd.read_csv(io.BytesIO(uploaded['dirlist.csv']),header=None)\n",
        "# days.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMPyX9RJ0W8E",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# data_encoding = \"ISO-8859-1\"\n",
        "# col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "# col_names = [\"id\", \"text\", \"created_at\", \"lang\", \"geo\",\"retweet_count\",'favorite_count','place']\n",
        "\n",
        "# import io\n",
        "# raw_df = pd.read_csv(io.BytesIO(uploaded['2020-03-05.csv']))\n",
        "# raw_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkRiefzlMy-i",
        "outputId": "4c1388d1-0e34-4306-90d6-e5b98d34839d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4.47 s, sys: 298 ms, total: 4.76 s\n",
            "Wall time: 5.13 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "dfs={}\n",
        "for day in days[0]:\n",
        "  if day !='dirlist.txt':\n",
        "      # use below line if dataset is in Drive\n",
        "      raw_df = pd.read_csv(f'/content/gdrive/My Drive/DVA/output/{day}')\n",
        "      # use below line if dataset is uploaded\n",
        "      # raw_df = pd.read_csv(io.BytesIO(uploaded[day]))\n",
        "      \n",
        "      # Dropping unnecessary columns\n",
        "      raw_df = raw_df.drop([\"geo\",\"place\"],axis=1)\n",
        "      # Filtering english only tweets\n",
        "      is_english = raw_df['lang']=='en'\n",
        "      df = raw_df[is_english].copy()\n",
        "      df = df.reset_index(drop=True)\n",
        "      dfs[day]=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRs3Kz9QIYrc",
        "outputId": "6a2031e0-abad-4d3a-c5de-0ec541f9cd79",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "156"
            ]
          },
          "execution_count": 21,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number of days in dataset\n",
        "len(dfs.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "p2UYzkTcRCKo",
        "outputId": "0f00f82e-9f85-4383-f24a-518e55aa56b5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9c65b5a9e8>"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "image/png": {
              "height": 254,
              "width": 422
            },
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Distribution of number of english tweets for each day in the dataset\n",
        "d=[df.shape[0] for df in dfs.values()]\n",
        "sns.distplot(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38BborD55wKv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Define the sentiment names\n",
        "sentiment_names = ['negative', 'positive','neutral']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joNdpSRSG4u4",
        "outputId": "dc58db8b-ddaf-4681-d7fe-1f852d8411ec",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
            "\r\u001b[K     |██████▍                         | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30kB 34.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40kB 33.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=1b0c75347d8d833d637762d859f32ba55987d08e60585a6c27f865a00397cb85\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install emoji --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-9b1X0EIysF"
      },
      "source": [
        "## Data Preprosseing and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nsU26Iu7LT0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "import emoji\n",
        "def preprocess_text(tweet):\n",
        "    tweet = emoji.demojize(tweet)\n",
        "    # remove character that cause new line when converting to csv\n",
        "    tweet = re.sub(r'\\r\\r|\\r|\\n\\n|\\n', '',tweet)\n",
        "    # replace links with 'url'\n",
        "    tweet = re.sub(r'((https?:\\/\\/)|(www\\.))[A-Za-z0-9.\\/]+', 'url',  tweet)\n",
        "    tweet = re.sub(r'[A-Za-z0-9]+.com', 'url',tweet)\n",
        "    # remove @users\n",
        "    tweet = re.sub(r'[@][A-Za-z0-9]+', '',tweet)\n",
        "    # remove non-ascii chars\n",
        "    tweet = ''.join([w for w in tweet if ord(w)<128])\n",
        "    # hastags: bert tokenizer handle such tokens\n",
        "    tweet = tweet.strip()\n",
        "    return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pc1aZTemFGcx",
        "outputId": "ca893565-4b6b-449e-94a1-805b4a26381f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Best Coronavirus-Inspired Street Art From Around the World- The Discoverer\\r\\rhttps://t.co/KoidF6pJPv'"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sample tweet before text processing\n",
        "dfs['2020-05-14.csv'].text[4404]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUoJgURaJM1T"
      },
      "source": [
        "### Perform text processing on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XheI9J2q9sAi",
        "outputId": "7d5f9113-8c4c-4e96-8174-3b4c872fad37",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 22min 35s, sys: 302 ms, total: 22min 35s\n",
            "Wall time: 22min 36s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for df in dfs.values():\n",
        "  df.text = df.text.apply(lambda x: preprocess_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M9iJYQVHfiKP",
        "outputId": "4fa3ae12-cef6-4afc-8bcd-8abd39f8f0f4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Best Coronavirus-Inspired Street Art From Around the World- The Discoverer\\r\\rurl'"
            ]
          },
          "execution_count": 38,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sample tweet after text processing\n",
        "dfs['2020-05-14.csv'].text[4404]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDmx7vePLtNU"
      },
      "source": [
        "## Text Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGpyM2bdLjqV"
      },
      "source": [
        "###Import tokenizer from Bert cased based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "538528da6c534d898228a096291bad3f",
            "d6c8d714417a4c3d964b7b2782207dbb",
            "15c6f23bda194c6e89b6445aea2bd37c",
            "b9b5e449dc134e1a85afc07f28d76cb0",
            "377854de1d494e36a5997b54a2fcd332",
            "e5beda54c4c74bb2b49dbdcfd06db4ee",
            "94367e553e31413397ca3f92a6b9c8c0",
            "5b32843bad7742e18aa9adf0fa3a7e41"
          ]
        },
        "id": "24qELjpj6Od4",
        "outputId": "47d9150b-be1e-407a-ad1f-46399ff7ea08",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "538528da6c534d898228a096291bad3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "BERT_model_type = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry0nznEp-LJ_",
        "outputId": "bf8518bd-dced-4bdf-f075-5259ad2f68a9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":man_facepalming_light_skin_tone: (May I add an additional correction? No one in New York died from Corona)...but man, the fact that we are guided by hunches and thoughts rather than numbers and facts is really mind boggling :exploding_head: #COVID19 #coronavirus url\n",
            "tensor([[  101,   131,  1299,   168,  1339, 12320,  5031,   168,  1609,   168,\n",
            "          2241,   168,  3586,   131,   113,  1318,   146,  5194,  1126,  2509,\n",
            "         22590,   136,  1302,  1141,  1107,  1203,  1365,  1452,  1121,  3291,\n",
            "         15789,   102]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "execution_count": 43,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TEST the tokenizer on one tweet from dataset\n",
        "test = preprocess_text(dfs['2020-03-05.csv'].text[6])\n",
        "print(test)\n",
        "max=32\n",
        "encoding = tokenizer.encode_plus(\n",
        "  test,\n",
        "  max_length=32,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  # pad_to_max_length=True,\n",
        "  padding='max_length',\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        "  truncation=True\n",
        ")\n",
        "print(encoding['input_ids'])\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ3oEdt5-To2",
        "outputId": "dbfc8a11-8354-417c-b2fa-2f7abc4f247c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 13min 8s, sys: 129 ms, total: 13min 8s\n",
            "Wall time: 13min 9s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Find the distribution of text length to select a maximum length for text encoding\n",
        "length_per_day=[]\n",
        "\n",
        "for df in dfs.values():\n",
        "  lenght_of_token = []\n",
        "  for txt in df.text:\n",
        "    tokens = tokenizer.encode(txt, max_length=512,truncation=True)\n",
        "    lenght_of_token.append(len(tokens))\n",
        "  length_per_day.append(lenght_of_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCnbxKupHd9O",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Plot the the distribution of text lengths\n",
        "for lenght_of_token in length_per_day:\n",
        "  sns.distplot(lenght_of_token)\n",
        "  plt.xlim([0, 146]);\n",
        "  plt.xlabel('Token count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYb36RdMJrP-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "max_length = 140"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvNtT8AEKDta",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Finction to encode texts into tensor of tokens\n",
        "class EncodeTweetData(Dataset):\n",
        "  def __init__ (self,tweets,tokenizer,max_length):\n",
        "    self.tweets = tweets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__ (self):\n",
        "    return len(self.tweets)\n",
        "\n",
        "  def __getitem__(self,item):\n",
        "    tweet_txt = str(self.tweets[item])\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "          tweet_txt,\n",
        "          max_length=self.max_length,\n",
        "          add_special_tokens=True,\n",
        "          return_token_type_ids=False,\n",
        "          pad_to_max_length=True,\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='pt',\n",
        "    )\n",
        "    return {\n",
        "          'tweet_text': tweet_txt,\n",
        "          'input_ids': encoding['input_ids'].flatten(),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suyCSY97PtdB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Function that performs text encoding and creat a dataloader\n",
        "def make_data_loader(df, tokenizer, max_len, batch_size):\n",
        "    ds = EncodeTweetData(\n",
        "    tweets = df.text.to_numpy(),\n",
        "    tokenizer = tokenizer,\n",
        "    max_length = max_len\n",
        "  )\n",
        "\n",
        "    return DataLoader(ds,batch_size=batch_size,num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDXtK5dsWQAr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Batch size of 16 is selected based on the assumption in the trainig and recommendions of BERT model authors\n",
        "batch_size = 16\n",
        "daily_data_loaders = {}\n",
        "# creat dictionary containig dataloaders for each day in our dataset\n",
        "for day,df in dfs.items():\n",
        "  daily_data_loaders[day] = make_data_loader(df, tokenizer, max_length, batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxpTS7fHQVSS"
      },
      "source": [
        "# Sentiment Analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q7ebldlVWRk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Class to perform sentiment classification\n",
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(BERT_model_type)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giWMdV6oQiQj"
      },
      "source": [
        "## Importing pre-trained BERT model:\n",
        "Model can be uploaded from local directory or automatically downloaded from the link in the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFC_R6xkSzcM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Use this cell if model needs to be uploaded. can upload different model state\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_LSO9USI0kI",
        "outputId": "39d3c8bf-4a6b-4551-cd07-b3fad590bddc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1afvHvYRK2qvOMk-oVF6KDYrIO6hpUAik\n",
            "To: /content/best_model_state_e1.pth\n",
            "433MB [00:02, 168MB/s]\n"
          ]
        }
      ],
      "source": [
        "# This is the best state of the trained model with one epoch (best_model_state_e1.pth)\n",
        "# to use any other model, upload the model in the above cell and comment the download code below\n",
        "!gdown --id 1afvHvYRK2qvOMk-oVF6KDYrIO6hpUAik\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "a80f595b866f46b9ae5f16fd38bb3486",
            "1079dc0f7bce4b2bb96da82d8da699e7",
            "45793b78e88040f5ad35c1d326b61c6c",
            "fccb1715230048e28d841d56e5060818",
            "d4a77f734a9e44ac8ba9176ca18ed5cd",
            "4414706adc424230bb5495ff8ca41800",
            "b954435b22604112a999fb0b605e1777",
            "73d1d155503b460eb2fa25a76fb13d8b",
            "c7f7f834f74844a5b7ce2f038d03a011",
            "a920c368d10841c1ae236c6f41e7965f",
            "11e936c99d5647b29466aeead6371c3b",
            "f0f333e79165495981f7102670df6ffa",
            "98d26d068eb0417db12840021eecb218",
            "38ea4193b2ae4264a5c692990218c2e3",
            "098bc8f296084080bf4d4752d9ff7fc6",
            "7171cd6c41bd4fd69ea7c2a688ab4bf6"
          ]
        },
        "id": "bIeJIBtcmZz0",
        "outputId": "f765e716-9924-4b1c-f981-f25867b00266",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a80f595b866f46b9ae5f16fd38bb3486",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7f7f834f74844a5b7ce2f038d03a011",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Loading the model and transfering to the GPU \n",
        "model = SentimentClassifier(len(sentiment_names))\n",
        "model.load_state_dict(torch.load('best_model_state_e1.pth'))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR1vYgp1bVmY"
      },
      "source": [
        "# Examples of sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS9fzM9lbPTZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Sample Text: Replace with your own for testing\n",
        "tweet_text =\"With Corona now in the picture I would have a multi year BADD sentence.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW763zyOaRHd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Using the tokenizer to encode the tweet:\n",
        "encoded_tweet= tokenizer.encode_plus(\n",
        "  tweet_text,\n",
        "  max_length=140,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")\n",
        "# Using model to get the sentiment probability and predition\n",
        "\n",
        "input_ids = encoded_tweet['input_ids'].to(device)\n",
        "attention_mask = encoded_tweet['attention_mask'].to(device)\n",
        "tweet_texts = []\n",
        "predictions = []\n",
        "prediction_probs = []\n",
        "real_values = []\n",
        "sentiment = []\n",
        "\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "  probs = F.softmax(output, dim=1)\n",
        "  max_prob = torch.max(probs,dim=1)\n",
        "  # assigning neutral tag to sentiments with less that 0.55 probability of \n",
        "  # being positive or negative\n",
        "  sentiment_pred = torch.where(max_prob[0]<0.55,2,prediction)\n",
        "  tweet_texts.extend(tweet_text)\n",
        "  predictions.extend(prediction)\n",
        "  prediction_probs.extend(probs)\n",
        "  sentiment.extend(sentiment_pred)\n",
        "\n",
        "  sentiment = torch.stack(sentiment).cpu()\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': sentiment_names,\n",
        "  'values': prediction_probs[0]\n",
        "})\n",
        "\n",
        "print(f'Tweet text: {tweet_text}')\n",
        "print(f'Sentiment  : {sentiment_names[sentiment]}')\n",
        "\n",
        "print(f'predictions  : {sentiment_names[predictions]}')\n",
        "print(f'prediction_probs  : {pred_df}')\n",
        "\n",
        "sns.barplot(x='values', y='class_names', data = pred_df, orient='h')\n",
        "plt.ylabel('sentiment')\n",
        "plt.xlabel('probability')\n",
        "plt.xlim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x79kd11md9XW"
      },
      "source": [
        "## Perfomming sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WRYa8MMVvX2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Function to get prediction and probability distribution of each tweet using the pretrained model\n",
        "def model_predict(model, data_loader):\n",
        "  #  setting dropout and batch normalization layers to evaluation mode\n",
        "  model = model.eval()\n",
        "  \n",
        "  tweet_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  sentiment = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"tweet_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probabilities = F.softmax(outputs, dim=1)\n",
        "\n",
        "      max_prob = torch.max(probabilities,dim=1)\n",
        "      # assigning neutral label to sentiments with less that 0.55 probability of \n",
        "      # being positive or negative\n",
        "      sentiment_pred = torch.where(max_prob[0]<0.55,2,preds)\n",
        "\n",
        "\n",
        "      tweet_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probabilities)\n",
        "      sentiment.extend(sentiment_pred)\n",
        "\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  sentiment = torch.stack(sentiment).cpu()\n",
        "\n",
        "  return tweet_texts, predictions, prediction_probs ,sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEuaExaEWJV-",
        "outputId": "1e6278a1-7932-4f94-8f6c-c0707233dd99",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2h 14min 55s, sys: 1h 44min 15s, total: 3h 59min 10s\n",
            "Wall time: 4h 18s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Perform sentiment analysis\n",
        "pred_dict ={}\n",
        "for day,data_loader in daily_data_loaders.items():\n",
        "  y_tweet_texts, y_pred, y_pred_probs,sentiment_pred = model_predict(model,data_loader)\n",
        "  pred_dict[day]=[y_tweet_texts, y_pred, y_pred_probs,sentiment_pred]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCpDMJDWXtS5"
      },
      "source": [
        "# Results and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "qpsMW3VSXWhH",
        "outputId": "989f47bc-c3b0-442b-9f0c-f50ff6104127",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_d5ff453f-36b9-45a4-8178-6ea059b4b1cb\", \"Overal_result.csv\", 5079)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 343 ms, sys: 102 ms, total: 446 ms\n",
            "Wall time: 329 ms\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "image/png": {
              "height": 343,
              "width": 342
            },
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%time\n",
        "# Test the output for March 5th:\n",
        "y_pred = pred_dict['2020-03-05.csv'][3]\n",
        "pred=y_pred.numpy()\n",
        "cls, freq = np.unique(pred, return_counts=True)\n",
        "count_dict = dict(zip(cls, freq))\n",
        "sns.displot(pred)\n",
        "sent_per_day={'date':[],'negative':[],'neutral':[],'positive':[]}\n",
        " \n",
        "\n",
        "for day,pred in pred_dict.items():\n",
        "  cls, freq = np.unique(pred[3], return_counts=True)\n",
        "  count_dict = dict(zip(cls, freq))\n",
        "  sent_per_day['date'].append(day)\n",
        "  sent_per_day['negative'].append(count_dict[0])\n",
        "  sent_per_day['positive'].append(count_dict[1])\n",
        "  sent_per_day['neutral'].append(count_dict[2])\n",
        "\n",
        "df_result=pd.DataFrame.from_dict(sent_per_day)\n",
        "\n",
        "df_result.to_csv('Overal_result.csv')\n",
        "\n",
        "# Downloading the overl result containg the frequency of each sentiment for each day in the datset\n",
        "files.download('Overal_result.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oueDPcpXKgCu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Creat a temproray file in the cloud to save the result of each day\n",
        "!mkdir 'result'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_LqTM3dC6mu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Compiling dictionary containig the result for all tweets and for each day separately\n",
        "def get_scalar(tensor_array):\n",
        "  return tensor_array.item()\n",
        "\n",
        "\n",
        "vec_get_scalar = np.vectorize(get_scalar)\n",
        "result_list=[]\n",
        "for day,pred in pred_dict.items(): \n",
        "  df=pd.DataFrame(pred,index=['text','prediction','probability','sentiment'])\n",
        "  df = df.transpose() \n",
        "  df['prob_negative']= df[['probability']].applymap(lambda x:x.numpy()[0])\n",
        "  df['prob_positive']= df[['probability']].applymap(lambda x:x.numpy()[1])\n",
        "  df['prob_neutral']= df[['probability']].applymap(lambda x:x.numpy()[2])\n",
        "  df=df.drop('probability',axis=1)\n",
        "  df[['prediction','sentiment']] = vec_get_scalar(df[['prediction','sentiment']])\n",
        "  df = df[['text','prob_negative','prob_positive','prob_neutral','prediction','sentiment']]\n",
        "\n",
        "  \n",
        "  df.to_csv(f'result/Result_{day}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJZD3yOOL9lv",
        "outputId": "83fa0a05-ad45-4063-b55f-65a68a3b99a9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/result/ (stored 0%)\n",
            "  adding: content/result/Result_2020-07-02.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-07-07.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-05-19.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-06-11.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-04-16.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-05-02.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-06-09.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-06-07.csv (deflated 69%)\n",
            "  adding: content/result/Result_2020-05-27.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-07-21.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-06-29.csv (deflated 66%)\n",
            "  adding: content/result/Result_2020-06-14.csv (deflated 76%)\n",
            "  adding: content/result/Result_2020-04-12.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-03-30.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-06-05.csv (deflated 64%)\n",
            "  adding: content/result/Result_2020-07-29.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-07-17.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-05-16.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-05-17.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-08-07.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-08-01.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-08-05.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-04-19.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-06-02.csv (deflated 69%)\n",
            "  adding: content/result/Result_2020-05-26.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-07-12.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-04-11.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-05-21.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-06-20.csv (deflated 69%)\n",
            "  adding: content/result/Result_2020-04-14.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-06-17.csv (deflated 66%)\n",
            "  adding: content/result/Result_2020-05-20.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-07-09.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-07-26.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-03-18.csv (deflated 72%)\n",
            "  adding: content/result/Result_2020-05-11.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-06-21.csv (deflated 71%)\n",
            "  adding: content/result/Result_2020-07-19.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-04-25.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-06-28.csv (deflated 71%)\n",
            "  adding: content/result/Result_2020-04-10.csv (deflated 57%)\n",
            "  adding: content/result/Result_2020-07-20.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-04-17.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-07-30.csv (deflated 64%)\n",
            "  adding: content/result/Result_2020-03-16.csv (deflated 69%)\n",
            "  adding: content/result/Result_2020-03-08.csv (deflated 74%)\n",
            "  adding: content/result/Result_2020-04-08.csv (deflated 66%)\n",
            "  adding: content/result/Result_2020-06-16.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-03-22.csv (deflated 73%)\n",
            "  adding: content/result/Result_2020-04-07.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-06-01.csv (deflated 67%)\n",
            "  adding: content/result/Result_2020-07-14.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-03-23.csv (deflated 71%)\n",
            "  adding: content/result/Result_2020-05-25.csv (deflated 64%)\n",
            "  adding: content/result/Result_2020-07-04.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-04-24.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-08-03.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-05-14.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-04-06.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-04-28.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-06-04.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-05-22.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-06-08.csv (deflated 66%)\n",
            "  adding: content/result/Result_2020-05-29.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-06-15.csv (deflated 71%)\n",
            "  adding: content/result/Result_2020-03-24.csv (deflated 70%)\n",
            "  adding: content/result/Result_2020-04-18.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-03-19.csv (deflated 70%)\n",
            "  adding: content/result/Result_2020-07-15.csv (deflated 67%)\n",
            "  adding: content/result/Result_2020-03-26.csv (deflated 64%)\n",
            "  adding: content/result/Result_2020-06-03.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-08-06.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-07-27.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-07-11.csv (deflated 66%)\n",
            "  adding: content/result/Result_2020-03-28.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-04-21.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-04-05.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-04-15.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-07-28.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-05-18.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-07-08.csv (deflated 67%)\n",
            "  adding: content/result/Result_2020-03-25.csv (deflated 64%)\n",
            "  adding: content/result/Result_2020-05-05.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-03-17.csv (deflated 77%)\n",
            "  adding: content/result/Result_2020-06-27.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-07-18.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-07-22.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-03-09.csv (deflated 70%)\n",
            "  adding: content/result/Result_2020-08-02.csv (deflated 66%)\n",
            "  adding: content/result/Result_2020-04-02.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-03-13.csv (deflated 73%)\n",
            "  adding: content/result/Result_2020-06-26.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-04-22.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-06-24.csv (deflated 68%)\n",
            "  adding: content/result/Result_2020-04-03.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-03-10.csv (deflated 71%)\n",
            "  adding: content/result/Result_2020-05-07.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-04-09.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-05-24.csv (deflated 67%)\n",
            "  adding: content/result/Result_2020-03-21.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-04-04.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-03-15.csv (deflated 71%)\n",
            "  adding: content/result/Result_2020-06-18.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-05-06.csv (deflated 57%)\n",
            "  adding: content/result/Result_2020-06-12.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-03-11.csv (deflated 70%)\n",
            "  adding: content/result/Result_2020-08-04.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-03-27.csv (deflated 67%)\n",
            "  adding: content/result/Result_2020-04-30.csv (deflated 57%)\n",
            "  adding: content/result/Result_2020-04-23.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-06-06.csv (deflated 66%)\n",
            "  adding: content/result/Result_2020-06-23.csv (deflated 68%)\n",
            "  adding: content/result/Result_2020-03-07.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-05-04.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-06-30.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-07-23.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-07-24.csv (deflated 60%)\n",
            "  adding: content/result/Result_2020-07-31.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-07-01.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-07-06.csv (deflated 67%)\n",
            "  adding: content/result/Result_2020-06-13.csv (deflated 64%)\n",
            "  adding: content/result/Result_2020-04-13.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-05-23.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-03-12.csv (deflated 72%)\n",
            "  adding: content/result/Result_2020-06-25.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-04-20.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-05-03.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-03-20.csv (deflated 70%)\n",
            "  adding: content/result/Result_2020-05-28.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-05-15.csv (deflated 56%)\n",
            "  adding: content/result/Result_2020-03-06.csv (deflated 70%)\n",
            "  adding: content/result/Result_2020-07-03.csv (deflated 63%)\n",
            "  adding: content/result/Result_2020-03-29.csv (deflated 69%)\n",
            "  adding: content/result/Result_2020-04-26.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-07-25.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-05-09.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-05-30.csv (deflated 67%)\n",
            "  adding: content/result/Result_2020-05-08.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-04-27.csv (deflated 57%)\n",
            "  adding: content/result/Result_2020-05-01.csv (deflated 57%)\n",
            "  adding: content/result/Result_2020-03-14.csv (deflated 77%)\n",
            "  adding: content/result/Result_2020-03-31.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-03-05.csv (deflated 78%)\n",
            "  adding: content/result/Result_2020-05-12.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-06-19.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-05-31.csv (deflated 75%)\n",
            "  adding: content/result/Result_2020-04-01.csv (deflated 65%)\n",
            "  adding: content/result/Result_2020-05-13.csv (deflated 58%)\n",
            "  adding: content/result/Result_2020-07-10.csv (deflated 61%)\n",
            "  adding: content/result/Result_2020-07-13.csv (deflated 66%)\n",
            "  adding: content/result/Result_2020-05-10.csv (deflated 62%)\n",
            "  adding: content/result/Result_2020-06-10.csv (deflated 64%)\n",
            "  adding: content/result/Result_2020-07-16.csv (deflated 66%)\n",
            "  adding: content/result/Result_2020-04-29.csv (deflated 59%)\n",
            "  adding: content/result/Result_2020-07-05.csv (deflated 69%)\n",
            "  adding: content/result/Result_2020-06-22.csv (deflated 68%)\n"
          ]
        }
      ],
      "source": [
        "# Creating a zip file containig all the results in the result folder and saving in the file.zip file\n",
        "!zip -r /content/file.zip /content/result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fHtLfAY_7s5g",
        "outputId": "e2698ac5-db31-473f-8910-c60d6f83632b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_68aac57f-f7a9-429b-bb66-eeca790850ef\", \"file.zip\", 123564234)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Downloading the final daily results\n",
        "files.download(\"/content/file.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fse9-QBLZEjc"
      },
      "source": [
        "# Plot: Sentiment frequency VS Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ShTanY30sSS6",
        "outputId": "40bc3fa3-e71c-45cd-a155-aae5ffd9bea9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6ebbcf11d0>"
            ]
          },
          "execution_count": 69,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "image/png": {
              "height": 270,
              "width": 408
            },
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "df_result.plot(xlabel='Day',ylabel='Frequency')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "UvzGC0PMVlom",
        "outputId": "e7497884-e8d6-449a-df1e-c21bc8718082",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>negative</th>\n",
              "      <th>neutral</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-05</td>\n",
              "      <td>1424</td>\n",
              "      <td>198</td>\n",
              "      <td>2034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-06</td>\n",
              "      <td>5104</td>\n",
              "      <td>472</td>\n",
              "      <td>3774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-07</td>\n",
              "      <td>6023</td>\n",
              "      <td>491</td>\n",
              "      <td>2755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-08</td>\n",
              "      <td>4482</td>\n",
              "      <td>410</td>\n",
              "      <td>4156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-09</td>\n",
              "      <td>5018</td>\n",
              "      <td>286</td>\n",
              "      <td>3982</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date  negative  neutral  positive\n",
              "0  2020-03-05      1424      198      2034\n",
              "1  2020-03-06      5104      472      3774\n",
              "2  2020-03-07      6023      491      2755\n",
              "3  2020-03-08      4482      410      4156\n",
              "4  2020-03-09      5018      286      3982"
            ]
          },
          "execution_count": 103,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_per_day={'date':[],'negative':[],'neutral':[],'positive':[]}\n",
        "for day,pred in pred_dict.items():\n",
        "  cls, freq = np.unique(pred[3], return_counts=True)\n",
        "  count_dict = dict(zip(cls, freq))\n",
        "  date=day[:-4]\n",
        "  # print(date)\n",
        "  sent_per_day['date'].append(date)\n",
        "  sent_per_day['negative'].append(count_dict[0])\n",
        "  sent_per_day['positive'].append(count_dict[1])\n",
        "  sent_per_day['neutral'].append(count_dict[2])\n",
        "\n",
        "df_result_2=pd.DataFrame.from_dict(sent_per_day)\n",
        "df_result_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEPi7zQRsDhH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT_Sentiment_Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "098bc8f296084080bf4d4752d9ff7fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1079dc0f7bce4b2bb96da82d8da699e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11e936c99d5647b29466aeead6371c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38ea4193b2ae4264a5c692990218c2e3",
            "max": 435779157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98d26d068eb0417db12840021eecb218",
            "value": 435779157
          }
        },
        "15c6f23bda194c6e89b6445aea2bd37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5beda54c4c74bb2b49dbdcfd06db4ee",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_377854de1d494e36a5997b54a2fcd332",
            "value": 213450
          }
        },
        "377854de1d494e36a5997b54a2fcd332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "38ea4193b2ae4264a5c692990218c2e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4414706adc424230bb5495ff8ca41800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45793b78e88040f5ad35c1d326b61c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4414706adc424230bb5495ff8ca41800",
            "max": 433,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4a77f734a9e44ac8ba9176ca18ed5cd",
            "value": 433
          }
        },
        "538528da6c534d898228a096291bad3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15c6f23bda194c6e89b6445aea2bd37c",
              "IPY_MODEL_b9b5e449dc134e1a85afc07f28d76cb0"
            ],
            "layout": "IPY_MODEL_d6c8d714417a4c3d964b7b2782207dbb"
          }
        },
        "5b32843bad7742e18aa9adf0fa3a7e41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7171cd6c41bd4fd69ea7c2a688ab4bf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d1d155503b460eb2fa25a76fb13d8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94367e553e31413397ca3f92a6b9c8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98d26d068eb0417db12840021eecb218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "a80f595b866f46b9ae5f16fd38bb3486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45793b78e88040f5ad35c1d326b61c6c",
              "IPY_MODEL_fccb1715230048e28d841d56e5060818"
            ],
            "layout": "IPY_MODEL_1079dc0f7bce4b2bb96da82d8da699e7"
          }
        },
        "a920c368d10841c1ae236c6f41e7965f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b954435b22604112a999fb0b605e1777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9b5e449dc134e1a85afc07f28d76cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b32843bad7742e18aa9adf0fa3a7e41",
            "placeholder": "​",
            "style": "IPY_MODEL_94367e553e31413397ca3f92a6b9c8c0",
            "value": " 213k/213k [00:00&lt;00:00, 838kB/s]"
          }
        },
        "c7f7f834f74844a5b7ce2f038d03a011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11e936c99d5647b29466aeead6371c3b",
              "IPY_MODEL_f0f333e79165495981f7102670df6ffa"
            ],
            "layout": "IPY_MODEL_a920c368d10841c1ae236c6f41e7965f"
          }
        },
        "d4a77f734a9e44ac8ba9176ca18ed5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "d6c8d714417a4c3d964b7b2782207dbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5beda54c4c74bb2b49dbdcfd06db4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0f333e79165495981f7102670df6ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7171cd6c41bd4fd69ea7c2a688ab4bf6",
            "placeholder": "​",
            "style": "IPY_MODEL_098bc8f296084080bf4d4752d9ff7fc6",
            "value": " 436M/436M [00:28&lt;00:00, 15.4MB/s]"
          }
        },
        "fccb1715230048e28d841d56e5060818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73d1d155503b460eb2fa25a76fb13d8b",
            "placeholder": "​",
            "style": "IPY_MODEL_b954435b22604112a999fb0b605e1777",
            "value": " 433/433 [00:00&lt;00:00, 1.69kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
